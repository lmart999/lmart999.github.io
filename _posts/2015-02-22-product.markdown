---
layout: post
comments: false
title:  "Product Science"
excerpt: "Science drives product development and growth."
date:   2015-02-22 10:19:00
---

###Preface

I’m interested in the tools that technology startups use to design, and measure the success of, products. This covers the most interesting points that I’ve come across from reading everything I can find about product management and experimentation within the product development cycle.

###Growth

What defines a technology startup? Paul Graham [defines a startup](http://www.paulgraham.com/growth.html) as a company designed to grow fast. Growth distinguishes startups from the millions of local service businesses launched every year. Startups typically aim for growth that is proportional to their current size: a fixed percentage of new users added per week will lead to a non-linear explosion in total user count over time.

###Why is growth important?

Consider two companies growing with a two-fold difference in rate. After one year, the first startup has passed the knee of its exponential and the difference in revenue is more than 10-fold. Many iconic startups have enjoyed this run-away growth due to a network effect. Consider OpenTable, an online restaurant reservation marketplace. As more restaurants entered the system, more diners followed. This attracted more restaurants, fueling a flywheel that has given OpenTable over [90% market share](http://www.gourmetmarketing.net/rezbook-review-new-opentable-competitor/).

<figure>
<img src="/assets/compare_growth.png" width="70%">
<figcaption>
</figcaption>
</figure>

###Growth parameters

***Market size*** 

Growth runs up against bounds. Consider Facebook’s growth ([data from Ben Foster's blog](http://www.benphoster.com/facebook-user-growth-chart-2004-2010/)). A trivial upper-bound is the world population. But, it’s more interesting to consider psychological bounds. As explained by [Alex Schultz](http://genius.com/Alex-schultz-lecture-6-growth-annotated), Facebook hit a wall ~50M users (dashed line).

<figure>
<img src="/assets/facebook.png" width="70%">
<figcaption>
</figcaption>
</figure>

At the time, no social network had surpassed 100M users. Some doubted whether it was possible. Of course, they did it. Chamath has a good [talk](https://www.youtube.com/watch?v=raIUQP71SBU) about what it took to get past this barrier. The point is that growth bounds exist, but they are not always obvious to identify.

***Acquisition*** 

Feedback is critical for fast growth: the number of new users should be proportional to the existing user base. Viral marketing math captures this feedback in a [loop](http://www.forentrepreneurs.com/lessons-learnt-viral-marketing/) that links existing users to new user acquisition. The viral loop can explain the hyper-growth observed for some startups. 

For example, Peter Theil [noted](http://blakemasters.com/post/22405055017/peter-thiels-cs183-startup-class-9-notes-essay) that Paypal had 7% daily growth at one point. This growth loop began with power sellers and buyers who executed many transactions per unit time. With cash incentives, Paypal also had a high conversion rate among those exposed to the product through power users.   

***Retention*** 

Acquisition and retention are independent parameters in the growth model. This is key. [Alex Schultz](http://genius.com/Alex-schultz-lecture-6-growth-annotated) explains how retention curves are used for modeling this. A retention curve captures the degradation rate in monthly active users following acquisition. In the worst cast, it flattens out to zero.
 
<figure>
<img src="/assets/retention.png" width="70%">
<figcaption>
</figcaption>
</figure>

I simulated a few scenarios to convince myself (I will link to iPython notebook). At best, growth is stifled when users churn. At worst, growth crashes out to zero. The intuition is pretty clear. Leaky buckets don’t need water. They need to be patched. Viral acquisition with a bad product will hemorrhage users.  

###Metrics

These growth parameters are often broken into more granular metrics. There are some [heuristics](http://engineering.pinterest.com/post/112720487359/4-steps-to-better-goals-and-metrics) for picking metrics (e.g., they should be MMOM - Meaningful, Measurable, Operational, and Motivational). Two general metric classes appear frequently in everything I've read:

* [Engagement metrics:](http://www.slideshare.net/adamnash/be-a-great-product-leader-airbnb-2013) These are ratios such as monthly active users (MAU)/total or DAU/MAU.
* [Conversion:](http://abovethecrowd.com/2013/10/02/conversion-the-most-important-internet-metric-of-all-revisited/) Users that perform a desired action divided by the total. 

###Product and growth

After reading everything I could find about growth, a key point emerged: product is the lever that moves the metrics. Product is a free parameter that a startup can play with in order to drive growth. With that in mind, it's interesting to understand the process that startups use to build products.

This brought me to some great writing on product management. [Adam Nash](http://blog.adamnash.com/2012/03/06/top-10-product-leadership-lessons/), CEO at Wealthfront, distills lessons on product leadership, which I further compress into picking a metric, developing a hypothesis, prioritizing features based upon the hypothesis, executing, and measuring success.

Good product teams seem effective at boiling a product down to the [one thing](https://medium.com/@dunn/get-one-thing-right-89390244c553) they want the user to do and finding the [one metric that matters](https://medium.com/@joshelman/the-only-metric-that-matters-ab24a585b5ea) for success. For example, the success metric at AirBnB is the [overall conversion rate](http://nerds.airbnb.com/experiments-at-airbnb/) between searching and booking.

With a metric, how to develop a thesis and product plan? Amazon [does this by working backwards](http://www.quora.com/What-is-Amazons-approach-to-product-development-and-product-management) from the customer. They write the press release prior to starting any product development. [Adam Nash](http://blog.adamnash.com/2009/07/22/guide-to-product-planning-three-feature-buckets/) and [Ian McAllister](http://www.quora.com/What-are-the-best-ways-to-prioritize-a-list-of-product-features) also have some good writing product feature types and prioritization strategy. 

I found this process surprisingly familiar: (1) Spend time thinking through the thesis on the front-end of a project, (2) [don't rely on costly experiments](https://medium.com/the-year-of-the-looking-glass/the-agony-and-ecstasy-of-building-with-data-56215764d67c) to lead the way, (3) pick the minimum number of experiments and simple analyses to test your thesis, (4) build re-useable analysis pipelines.  

<figure>
<img src="/assets/product_cycle.png" width="90%">
<figcaption>
</figcaption>
</figure>

Feedback is especially important. In the sciences, iteration and learning across projects leads to what they call a ***taste for good problems***. This is analogous to good product management instincts or "spidey sense". So, what are the experimental learning systems built around the product cycle? And are they needed? Is the coupling between product/features and metrics just obvious?

###Experimental Schema and Design

There are at least two important considerations for experimental design.

***External factors***

A key challenge is that the feedback loop between product/features and the metrics is not always obvious. The outside world has an effect on metrics! Controlled experiments can be used to isolate the effects of a features on a metric from all of the external factors that could also influence the metric.

<figure>
<img src="/assets/cycle.png" width="100%">
<figcaption>
</figcaption>
</figure>

In addition to the metric of interest, external factors should also be logged in a controlled experiment. With this schema captured, users in both test (subject to a feature change) and control cohorts can be aggregated by these attributes to isolate effects of the feature change from the external factors.

<figure>
<img src="/assets/schema.png" width="100%">
<figcaption>
</figcaption>
</figure>

***The funnel***

It's also critical to consider the funnel to conversion. Each step may introduce frictions that affect the metric! [Jan Overgoor](http://nerds.airbnb.com/experiments-airbnb/), a data scientist at AirBnB, has a great blog post that discusses this: in addition to capturing external factors, experiments can be partitioned by specific steps in the funnel.

<figure>
<img src="/assets/funnel_airbnb.png" width="70%">
<figcaption>
</figcaption>
</figure>

Dan McKinley, an Engineering at Etsy and now Stripe, has a great talk on [product design for continuous experimentation](http://www.slideshare.net/danmckinley/design-for-continuous-experimentation) that extends this point: it's hard to interpret experiments if (1) many feature changes are made simultaneously or (2) if a feature change affects many parts of the funnel simultaneously. 

He talks about a failed experiment at Etsy: it involved a "monolithic" roll-out of a new features (the infinite scroll that automates loading of new items). This lead to fewer impressions and fewer purchases. When the feature failed, de-bugging was challenging because so much had been changed at once. 

A set of smaller, carefully designed experiments showed that the basic premises (e.g., more and faster results are good) were wrong. He goes on to discuss to discuss a more successful product launch: staged experiments gave them clear opportunity to test each hypotheses and, if needed, alter course.

<figure>
<img src="/assets/funnel.png" width="100%">
<figcaption>
</figcaption>
</figure>

###Instrumentation

With that in mind, how are experiments done? Logs are the substrate of experiments. They capture information specified by the schema. For example, [this blog post from Pinterest](http://engineering.pinterest.com/post/95378137929/scalable-a-b-experiments-at-pinterest) nicely explains how the schema is captured in different log files and later aggregated accordingly.

Logs are so central that [Jay Kreps](http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying), data scientist / engineering at LinkedIN (now CEO at Confluent), has an excellent post about the log and pipelines to process them. 

Data pipelines have producers and consumers. Producers write logs that may contain web events or physical data (e.g., RFID and IoT). Consumers process the logs. But because event data can be orders of magnitude larger than traditional database uses, the structure of data pipelines can be [complex](http://www.slideshare.net/charmalloc/current-and-future-of-apache-kafka).

<figure>
<img src="/assets/datapipeline_complex.png" width="100%">
<figcaption>
</figcaption>
</figure>

To address this, Jay and colleagues developed a central log that (1) is as a buffer between producers and consumers, (2) allows for subscription by systems (e.g., Hadoop) that consume data at different rates, and (3) allows new data systems to connect to all others with a single pipeline.

The outputs from the central log are then suitable for batch ETL into systems that allow analysis: system-specific non-sense is removed and the data is restructured for for warehousing (e.g., [STAR](http://en.wikipedia.org/wiki/Star_schema) schema) and admits to experimental analysis (simple counting, aggregation, and filtering). 

<figure>
<img src="/assets/log.png" width="100%">
<figcaption>
</figcaption>
</figure>

Logging agents, such as [`Singer`](http://www.slideshare.net/DiscoverPinterest/singer-pinterests-logging-infrastructure), sit on application servers. They collect event logs and ship them to a centralized repository. The centralized repository, such as [`Kafka`](http://engineering.linkedin.com/kafka/kafka-linkedin-current-and-future), consumes events and moves them to subscribers. `Kafka` powers event stream pipeline for [many companies](https://cwiki.apache.org/confluence/display/KAFKA/Powered+By).

For batch processing, storage systems like [`HDFS`, `HBase`,](http://smartdatacollective.com/mtariq/120791/hadoop-toolbox-when-use-what) and `S3` can be tied to relational databases such as `Hive`, which provides [scalable](http://www.quora.com/What-makes-Amazon-Redshift-faster-than-Hive) SQL-like functionality on top of a Hadoop cluster, and [`Redshift`](http://aws.amazon.com/redshift/), a SQL database on `S3` that [can](http://engineering.pinterest.com/post/75186894499/powering-interactive-data-analysis-by-redshift) have better performance than `Hive`).

For real-time processing, steam data processing services like `Storm` or `Spark streaming` are [comparable](http://www.slideshare.net/ptgoetz/apache-storm-vs-spark-streaming) and may connect to real-time data storage, such as [`HBase`](https://www.xplenty.com/blog/2014/05/hive-vs-hbase/), a NoSQL key-value store, or `MemSQL` for real-time databases for transactions and analytics.

For process coordination, [`Mesos`](http://www.slideshare.net/charmalloc/apache-kafka-hdfs-accumuloonmesos) joins multiple physical resources into a single virtual resource. It schedules CPU and memory resources across the cluster using `Zookeeper`, a centralized configuration manager. Frameworks, such as Hadoop, manage (e.g., start / stop) applications on `Mesos`.

<figure>
<img src="/assets/architecture.png" width="70%">
<figcaption>
</figcaption>
</figure>

###Pipelines

These building blocks for experimental pipelines are largely open-source and un-bundled into a collection of services. Companies can mix and match this instrumentation in different ways. For example, [Pinterest](http://engineering.pinterest.com/post/95378137929/scalable-a-b-experiments-at-pinterest) has a nice overview of their experimental `A/B test` pipeline that merges different log types to group metrics per experimental group, segmentation (e.g., gender), and days-in the experiment.

The scale of the data (`20TB` of experiment metric data every six months) requires a large-scale system (`HBase`) for batch processing. The `HBase` keys contain the schema needed to perform the required segmentation and different metrics are just captured in different column families.

Batch processing on the data in `HBase` is done daily and results are pushed to Dashboards for visualization. In order to improve the success rate of experiments, they also have a real-time pipelines (with `Storm` and [`Spark`](http://engineering.pinterest.com/post/111380432054/real-time-analytics-at-pinterest)) that can perform operations as the experiment is running.

<figure>
<img src="/assets/pinterest_overall.png" width="100%">
<figcaption>
</figcaption>
</figure>

###Controlled Expariments

Let's assume everything above has been done: we've designed an experiment, split into test and control cohorts, and captured logs with our metrics / schema using instrumentation described above. What now? We need a testing framework to rigorously evaluate whether there is a difference in the metric between the two.

Many metrics are binary: a user did something or they didn't. There are a few different kinds of statistical tests for binary data (`Chi-Squared`, `Z-test`, or `N-1 Chi-Squared test`). These tests assume a null-hypothesis (no difference between cohorts) and compute the probability of observing the observed effect assuming the null-hypothesis is true (a `p-value`).

But, P-values are poorly understood. Here's the problem: they are a function of sample size and effect size. [Jan Overgoor](http://nerds.airbnb.com/experiments-airbnb/) shows this nicely. They ran a test that computes a rolling P-value (bottom) for the treatment effect (top). Large effects ***randomly*** observed early on are called "significant," but the sample size is small. Over time, the effect curve converges to zero.

<figure>
<img src="/assets/airbnb_test.png" width="100%">
<figcaption>
</figcaption>
</figure>

As shown, tests can fail by identifying false positives (early timepoints above, Type I error) or false negative (Type II error). There's a few free parameters to combat these issues, including [Power](http://meera.snre.umich.edu/plan-an-evaluation/related-topics/power-analysis-statistical-significance-effect-size) (probability that you will reject the null hypothesis and avoid Type II error), sample size, expected effect size, and `p-value` threshold.

The ideal approach is to pre-determine the expected effect size, desired power, and P-value threshold prior to the experiment. If we fix power (0.8) and `p-value` threshold (0.05), then we just need to sweep across a range of expected effect sizes and compute the required sample sizes needed for enrollment (e.g., using [calculators](http://www.evanmiller.org/ab-testing/sample-size.html)).

Given an expected effect size, we can also compute a binomial parameter with confidence intervals using the cohort data. This parameter indicates the probability of conversion for each cohort. We expect that our feature influences changes this parameter. We can simply compare computed parameters between cohorts in order to test this.

Using data provided by Jason Davis, [a software/data engineer Etsy](http://people.stern.nyu.edu/ja1517/data/dsatetsy.pdf), I ran a simulation below: given an expected effective size (conversion goes from 2% to 2.04%) and user enrollment of 500k per day, the time needed to see the effect is 7 days (> 3.5M visits).

<figure>
<img src="/assets/ci.png" width="100%">
<figcaption>
</figcaption>
</figure>

An [intuitive and inverse](http://www.measuringu.com/blog/ab-testing.php) relationship between effect size and sample size falls out: you need more data to observe subtle effects! And for cases in which the effect size is unknown, [Jan](http://nerds.airbnb.com/experiments-airbnb/) mentions how dynamic (in time) p-value thresholds can be to determine whether or not an early result is worth investigating.

###Framework for Controlled Expariments

***(1) Free experiments to develop ideas***

My favorite kind of experiments are the ones that cost nothing: that is, quick data mining or back-of-the-envelope work at the experimental design phase in order to help clarify the hypothesis! For example, if we want to build a feature to reduce churn, we might be challenged by the fact that there are many things that may influence the metric.

<figure>
<img src="/assets/user_equ.png" width="60%">
<figcaption>
</figcaption>
</figure>

Examining historical data (that already exists) may be one way to suss this out rather quickly and improve clarity. And clarity is something that seems to unite successful product and growth teams. For Facebook, it was friend count. Zuck talks about getting people to 10 friends in 14 days. Josh Elman, product lead for growth at Twitter, explains that they focused on visits, with [7 times per month](https://medium.com/@joshelman/the-only-metric-that-matters-ab24a585b5ea) the threshold.

Once a team understands the product feature that strongly separates users based upon a key metric, then controlled experiments (as explained above) can be centered around this core insight.

<figure>
<img src="/assets/dist.png" width="70%">
<figcaption>
</figcaption>
</figure>

***(2) Exparimental Design***

* Define the metric.
* Define the (product/feature) hypothesis.
* Define schema to capture metric and confouding factors.
* Stage expariments based upon the conversion funnel.

***(3) Instrumentation***

* Design the simplest possible instrumentation scheme. 
* Make it re-usable so that it can be applied to many expariments.

***(4) Data collection, analysis, and reporting***

* In order to isolate feature effects, group on schema attributes.
* Ensure grouped samples are large enough to resolve the effect size.
* If computing a rolling P-value, use dynamic cutoffs.
* Also consider regression (generalized linear models) to avoid data slicing.
* Build intuitive reporting [dashboards](http://nerds.airbnb.com/experiment-reporting-framework/) and [visualizations](http://engineering.pinterest.com/post/86533331849/how-pinterest-drives-sustainable-growth

###Final Comments

Controlled expariments have been used for centries to better understand our physical world. But they have been largely directed at "model systems," which can be easily manipulated. For larger-scale phenomea, controlled expariments have been very hard to do. Scientists have relied on (econometric) frameworks (e.g., difference in differences) to approximate controlled experiments in these cases.

The advent of the web made it possible to conduct massive controlled expariments to improve our virtual experinces. The injection of software into [nearly every category](http://www.wsj.com/articles/SB10001424053111903480904576512250915629460) and means that it's possible to build learning loops that improve product and user experinces across a broader array of virual activity. 

The recent emergence of mobile extended the reach of the web. [Now, full-stack startups](http://a16z.com/2015/01/22/the-full-stack-startup/) are injecting software deep into the physical world. For the first time, it's possible to run massive controlled expariments to improve physical experinces. Companies like Uber have proven that focus on product can vastly grow markets for services and [alter the flow of cities](http://blog.uber.com/city-future). 

In the case of Uber, their full-stack digital grid is an unprecedented experimental machine. Using product science to improve the existing services (specifically POOL) has [interesting knock-on effects for cities](http://abovethecrowd.com/2015/01/30/ubers-new-bhag-uberpool/). Controlled exparimentation will also likely usher in new experinces; their exparimentation with on-demand [flu shots](http://blog.uber.com/health), [weddings](http://blog.uber.com/uberwedding), and [logistics](http://blog.uber.com/RUSH) all point to the fact that it's possible ask questions on a scale that no one else can.

###Note

I'm doing a PhD at Stanford and have no affiliation with any companies mentioned in this article.